{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TwoLayerNN(object):\n",
    "    \"\"\"\n",
    "                       Layer 1                   Layer 2                   Loss\n",
    "                   ___________________     ______________________   ___________________\n",
    "    Input (NxD) > {Hidden (DxH) > ReLU} > {Output (HxC) > Softmax} > Cross Entropy Loss\n",
    "    \n",
    "    where:\n",
    "        N: Number of instances of input\n",
    "        D: Dimensions of the input\n",
    "        H: Num hidden layer neurons\n",
    "        C: Num classes\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, std=1e-4):\n",
    "        \n",
    "        # Set the dimensions\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # Weights initialization (naive)\n",
    "        self.W1 = np.random.rand((input_dim, hidden_dim)) * std\n",
    "        self.b1 = np.ones((hidden_dim,)) * 0.001 # Slight positive bias values to help ReLU units\n",
    "        \n",
    "        self.W2 = np.random.rand((hidden_dim, output_dim)) * std\n",
    "        self.b2 = np.zeros((output_dim,))\n",
    "        \n",
    "        #######################################################################\n",
    "        # TODO Use Xavier weights initialiation instead\n",
    "        #######################################################################\n",
    "        \n",
    "    def loss(self, X, y, reg_loss_factor=0.0):\n",
    "        \"\"\"\n",
    "        Given the batch {X, y} and the current set of weights, we calculate the cross entropy \n",
    "        loss. We also calculate the gradient of the loss wrt the weights and return them.\n",
    "        \n",
    "        We add L2 Regularization loss, with reg_loss acting as the weight of that loss.\n",
    "        \"\"\"\n",
    "        \n",
    "        num_train = X.shape[0]\n",
    "        \n",
    "        # Forward pass\n",
    "        \n",
    "        # Layer 1: Calculate the scores and pass it through ReLU\n",
    "        # Dimension of output will be [num_train x hidden_dim]\n",
    "        layer1_relu_output = np.maximum(0, X.dot(self.W1) + self.b1)\n",
    "        \n",
    "        # Layer 2: Calculate the scores and pass it through Softmax\n",
    "        # Dimension of output will be [num_train x output_dim]\n",
    "        layer2_temp = layer1_relu_output.dot(self.W2) + self.b1\n",
    "        layer2_temp = layer2_temp - np.max(layer2_temp, axis=1).reshape((-1, 1)) # For numerical stability\n",
    "        layer2_temp_exp = np.exp(layer2_temp)\n",
    "        layer2_temp_exp_sums = np.sum(layer2_temp_exp, axis=1).reshape((-1, 1))\n",
    "        layer2_softmax_output = layer2_temp_exp / layer2_temp_exp_sums\n",
    "        \n",
    "        # Calculate the cross entropy loss with regularization loss\n",
    "        data_loss = -np.log(layer2_softmax_output[np.arange(num_train), y])\n",
    "        data_loss = np.sum(loss) / num_train\n",
    "        reg_loss  = 0.5 * reg_loss_factor * np.sum(self.W1 * self.W1)\n",
    "        reg_loss += 0.5 * reg_loss_factor * np.sum(self.W2 * self.W2)\n",
    "        loss = data_loss + reg_loss\n",
    "        \n",
    "        # Backward pass\n",
    "        \n",
    "        # Ok, so now we have a single number in 'loss', and we have to find the contribution \n",
    "        # of each of the weights and biases on this loss\n",
    "        \n",
    "        # Through output layer\n",
    "        probs = layer2_softmax_output.copy()\n",
    "        probs[np.arange(num_train), y] -= 1\n",
    "        probs /= num_train\n",
    "        dW2 = layer1_relu_output.T.dot(probs)\n",
    "        dW2 += reg_loss_factor * self.W2\n",
    "        db2 = np.sum(probs, axis=0)\n",
    "        \n",
    "        # Through hidden layer\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
